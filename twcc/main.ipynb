{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nilearn.image import reorder_img, resample_img\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import plot_image\n",
    "import transforms\n",
    "import models\n",
    "import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "train_dir = dict()\n",
    "train_dir['mask'] = r\"label_samseg/*\"\n",
    "test_dir = dict()\n",
    "test_dir['image'] = r'candi_oasis_aseg/raw123/*'\n",
    "test_dir['mask'] = r'candi_oasis_aseg/label123/*'\n",
    "\n",
    "label_all = dict()\n",
    "label_all['synsg'] = (\n",
    "    2,3,4,5,7,8,10,11,12,13,14,15,16,17,18,24,26,28,31,\n",
    "    41,42,43,44,46,47,49,50,51,52,53,54,58,60,63,77\n",
    ")\n",
    "\n",
    "# dgm_labels = [0, 10, 49, 11, 50, 12, 51, 13, 52, 17, 53, 18, 54]\n",
    "\n",
    "label_transforms = transforms.Compose([\n",
    "    transforms.RandomSkullStrip(),\n",
    "    transforms.LinearDeform(scales=(0.8, 1.2), degrees=(-20, 20), shears=(-0.015, 0.015), trans=(-30, 30)),\n",
    "#     transforms.NonlinearDeform(max_std=4),\n",
    "    transforms.NonlinearDeformTio(),\n",
    "    transforms.RandomCrop(160)\n",
    "])\n",
    "\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.GMMSample(mean=(0, 255), std=(0, 35)),\n",
    "    transforms.RandomBiasField(max_std=0.6),\n",
    "    transforms.Rescale(),\n",
    "    transforms.GammaTransform(std=0.4),\n",
    "    transforms.RandomDownSample(max_slice_space=9, alpha=(0.95, 1.05), r_hr=1)\n",
    "])\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_voxel(data_nib, voxelsize, target_shape=None, interpolation='continuous'):\n",
    "    affine = data_nib.affine\n",
    "    target_affine = affine.copy()\n",
    "\n",
    "    factor = np.zeros(3)\n",
    "    for i in range(3):\n",
    "        factor[i] = voxelsize[i] / \\\n",
    "            np.sqrt(affine[0, i]**2 + affine[1, i]**2 + affine[2, i]**2)\n",
    "        target_affine[:3, i] = target_affine[:3, i]*factor[i]\n",
    "\n",
    "    new_nib = resample_img(data_nib, target_affine=target_affine,\n",
    "                           target_shape=target_shape, interpolation=interpolation)\n",
    "\n",
    "    return new_nib\n",
    "\n",
    "def nib_to_tensor(input_nib, resample='continuous'):\n",
    "#     input_nib_resp = reorder_img(input_nib, resample=resample)\n",
    "#     input_nib_resp = resample_voxel(input_nib_resp, (1, 1, 1), interpolation=resample)\n",
    "    \n",
    "#     vol = torch.from_numpy(input_nib_resp.get_fdata()).float()[None, None, ...]\n",
    "    vol = torch.from_numpy(input_nib.get_fdata()).float()[None, None, ...]\n",
    "    return vol, input_nib_resp.affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetData(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir='', mask_dir='', mode='both'):\n",
    "        self.image_ffs = glob.glob(image_dir)\n",
    "        self.mask_ffs = glob.glob(mask_dir)\n",
    "        self.mask_ffs.sort()\n",
    "        self.image_ffs.sort()\n",
    "        \n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mask_ffs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = dict()\n",
    "        \n",
    "        if(self.mode == 'image' or self.mode == 'both'):\n",
    "            data['image'] = nib.load(self.image_ffs[index])\n",
    "            \n",
    "        if(self.mode == 'mask' or self.mode == 'both'):\n",
    "            data['mask'] = nib.load(self.mask_ffs[index])\n",
    "            \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Functions    \n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = torch.nn.functional.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(focal_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(focal_loss)\n",
    "        else:\n",
    "            return focal_loss\n",
    "    \n",
    "class SoftDiceLossWithLogit(torch.nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(SoftDiceLossWithLogit, self).__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_logit = torch.nn.functional.softmax(y_pred, dim=1)\n",
    "        intersection = (y_logit * y_true).sum(dim=(2, 3, 4))\n",
    "        union = y_logit.sum(dim=(2, 3, 4)) + y_true.sum(dim=(2, 3, 4))\n",
    "        return 1 - torch.mean((2 * intersection + self.smooth) / (union + self.smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset has 991 Nifti images.\n",
      "Test Dataset has 123 Nifti images.\n",
      "Model predicts 35 labels.\n"
     ]
    }
   ],
   "source": [
    "train_set = GetData(mask_dir=train_dir['mask'], mode='mask')\n",
    "valid_set = GetData(test_dir['image'], test_dir['mask'], mode='both')\n",
    "print(f\"Training Dataset has {len(train_set)} Nifti images.\")\n",
    "print(f\"Test Dataset has {len(valid_set)} Nifti images.\")\n",
    "\n",
    "pred_labels = label_all['synsg']\n",
    "learning_rate = 1e-4\n",
    "model = models.Unet3D(1, len(pred_labels), 24).to(device)\n",
    "print(f\"Model predicts {len(pred_labels)} labels.\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = SoftDiceLossWithLogit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load checkpoint ? [y/n]n\n",
      "starting step:  0\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"save\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "bestmodel_path = os.path.join(save_dir, \"bestmodel.pth\")\n",
    "checkpoint_path = os.path.join(save_dir, \"checkpoint.pth.tar\")\n",
    "\n",
    "if input(\"Load checkpoint ? [y/n]\") == \"y\":\n",
    "    checkpoint = torch.load(os.path.join(save_dir, \"checkpoint.pth.tar\"))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_step = checkpoint['step']\n",
    "    loss_list = checkpoint['loss']\n",
    "    dice_list = checkpoint['dice']\n",
    "    best_score = checkpoint['best_score']\n",
    "    print(\"load checkpoint succesfully\")\n",
    "else:\n",
    "    start_step = 0\n",
    "    dice_list = []\n",
    "    loss_list = []\n",
    "    best_score = -1\n",
    "    \n",
    "print(\"starting step: \", start_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataset):\n",
    "    def get_dice(mask1, mask2):\n",
    "        dice = torch.sum(mask1 & mask2) * 2\n",
    "        dice = dice / (1e-6 + torch.sum(mask1) + torch.sum(mask2))\n",
    "        return dice\n",
    "    \n",
    "    deepgm_to_aseg = [0, 10, 49, 11, 50, 12, 51, 13, 52, 17, 53, 18, 54]\n",
    "    \n",
    "    total_scores = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataset, desc=\"validating: \"):\n",
    "            image, image_affine = nib_to_tensor(data[\"image\"], resample='continuous')\n",
    "            mask, label_affine = nib_to_tensor(data[\"mask\"], resample='nearest')\n",
    "            \n",
    "            image = image.to(device)\n",
    "            mask = mask.to(device)\n",
    "            pred_mask = torch.argmax(\n",
    "                torch.nn.functional.softmax(model(image), dim=1)[0, ...],\n",
    "                dim=0\n",
    "            )\n",
    "\n",
    "            pred_scores = 0\n",
    "            for deepgm in range(1, 12+1):\n",
    "                pred_scores += get_dice(\n",
    "                    pred_mask == deepgm,\n",
    "                    mask == deepgm\n",
    "                ).item()\n",
    "                \n",
    "            total_scores += (pred_scores / 12)\n",
    "            \n",
    "    return total_scores / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Steps: 20000, Learning Rate: 0.0001\n",
      "Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Loss Function: SoftDiceLossWithLogit()\n",
      "Start Training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  12%|█▏        | 24/200 [02:08<15:44,  5.37s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mrange\u001b[39m(savestep), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m---> 17\u001b[0m     mask, _ \u001b[38;5;241m=\u001b[39m \u001b[43mnib_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnearest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     mask \u001b[38;5;241m=\u001b[39m label_transforms(mask\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     20\u001b[0m     label \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39msplit_labels(mask, pred_labels)\n",
      "Cell \u001b[0;32mIn[14], line 17\u001b[0m, in \u001b[0;36mnib_to_tensor\u001b[0;34m(input_nib, resample)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnib_to_tensor\u001b[39m(input_nib, resample\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontinuous\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 17\u001b[0m     input_nib_resp \u001b[38;5;241m=\u001b[39m \u001b[43mreorder_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_nib\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     input_nib_resp \u001b[38;5;241m=\u001b[39m resample_voxel(input_nib_resp, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), interpolation\u001b[38;5;241m=\u001b[39mresample)\n\u001b[1;32m     20\u001b[0m     vol \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(input_nib_resp\u001b[38;5;241m.\u001b[39mget_fdata())\u001b[38;5;241m.\u001b[39mfloat()[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nilearn/image/resampling.py:821\u001b[0m, in \u001b[0;36mreorder_img\u001b[0;34m(img, resample)\u001b[0m\n\u001b[1;32m    819\u001b[0m     Q, R \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mqr(affine[:\u001b[38;5;241m3\u001b[39m, :\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m    820\u001b[0m     target_affine \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(np\u001b[38;5;241m.\u001b[39mabs(np\u001b[38;5;241m.\u001b[39mdiag(R))[np\u001b[38;5;241m.\u001b[39mabs(Q)\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)])\n\u001b[0;32m--> 821\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresample_img\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_affine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_affine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    825\u001b[0m axis_numbers \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(np\u001b[38;5;241m.\u001b[39mabs(A), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    826\u001b[0m data \u001b[38;5;241m=\u001b[39m _get_data(img)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nilearn/image/resampling.py:673\u001b[0m, in \u001b[0;36mresample_img\u001b[0;34m(img, target_affine, target_shape, interpolation, copy, order, clip, fill_value, force_resample)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[38;5;66;03m# Iterate over a set of 3D volumes, as the interpolation problem is\u001b[39;00m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;66;03m# separable in the extra dimensions. This reduces the\u001b[39;00m\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;66;03m# computational cost\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndindex(\u001b[38;5;241m*\u001b[39mother_shape):\n\u001b[0;32m--> 673\u001b[0m         \u001b[43m_resample_one_img\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mall_img\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mind\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m            \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m            \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtarget_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m            \u001b[49m\u001b[43minterpolation_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m            \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresampled_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mall_img\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mind\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minput_img_is_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clip:\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;66;03m# force resampled data to have a range contained in the original data\u001b[39;00m\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;66;03m# preventing ringing artefact\u001b[39;00m\n\u001b[1;32m    687\u001b[0m     \u001b[38;5;66;03m# We need to add zero as a value considered for clipping, as it\u001b[39;00m\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# appears in padding images.\u001b[39;00m\n\u001b[1;32m    689\u001b[0m     vmin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(data\u001b[38;5;241m.\u001b[39mmin(), \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nilearn/image/resampling.py:306\u001b[0m, in \u001b[0;36m_resample_one_img\u001b[0;34m(data, A, b, target_shape, interpolation_order, out, copy, fill_value)\u001b[0m\n\u001b[1;32m    304\u001b[0m         warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mUserWarning\u001b[39;00m)\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;66;03m# The resampling itself\u001b[39;00m\n\u001b[0;32m--> 306\u001b[0m     \u001b[43maffine_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_not_finite:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;66;03m# Suppresses warnings in https://github.com/nilearn/nilearn/issues/1363\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/ndimage/_interpolation.py:614\u001b[0m, in \u001b[0;36maffine_transform\u001b[0;34m(input, matrix, offset, output_shape, output, order, mode, cval, prefilter)\u001b[0m\n\u001b[1;32m    611\u001b[0m     _nd_image\u001b[38;5;241m.\u001b[39mzoom_shift(filtered, matrix, offset\u001b[38;5;241m/\u001b[39mmatrix, output, order,\n\u001b[1;32m    612\u001b[0m                          mode, cval, npad, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 614\u001b[0m     \u001b[43m_nd_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeometric_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "savestep = 200\n",
    "steps =  100 * savestep\n",
    "# savestep = 2\n",
    "# steps = 2 * savestep\n",
    "\n",
    "print(f\"Number of Steps: {steps}, Learning Rate: {learning_rate}\")\n",
    "print(f\"Optimizer: {optimizer}\")\n",
    "print(f\"Loss Function: {loss_fn}\")\n",
    "print(\"Start Training...\\n\")\n",
    "torch.cuda.empty_cache()\n",
    "idx = 0\n",
    "model.train()\n",
    "for i in range(steps//savestep):\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(range(savestep), desc=\"training\")\n",
    "    for j in pbar:\n",
    "        mask, _ = nib_to_tensor(train_set[idx]['mask'], resample=\"nearest\")\n",
    "        mask = label_transforms(mask.to(device))\n",
    "        \n",
    "        label = transforms.split_labels(mask, pred_labels)\n",
    "        image = image_transforms(mask)\n",
    "        \n",
    "\n",
    "        pred = model(image)\n",
    "        loss = loss_fn(pred, label)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        idx += 1\n",
    "        idx = idx if idx < len(train_set) else 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_loss /= savestep\n",
    "        dice_score = validate(model, valid_set)\n",
    "        \n",
    "        step = (i+1) * savestep + start_step\n",
    "        print(f\"[{step}/{steps + start_step}] Dice: {dice_score}, Loss: {total_loss}\")\n",
    "        \n",
    "        if best_score == -1 or dice_score > best_score:\n",
    "            best_score = dice_score\n",
    "            torch.save(model, bestmodel_path)\n",
    "            print(\"! save best model !\")\n",
    "        \n",
    "        loss_list.append(total_loss)\n",
    "        dice_list.append(dice_score)\n",
    "\n",
    "        # save checkpoint\n",
    "        torch.save({\n",
    "            'step': step,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss_list,\n",
    "            'dice': dice_list,\n",
    "            'best_score': best_score\n",
    "        }, checkpoint_path)\n",
    "        print(\"=> save checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(os.path.join(save_dir, \"checkpoint.pth.tar\"))\n",
    "losses = checkpoint[\"loss\"]\n",
    "scores = checkpoint[\"dice\"]\n",
    "x = np.arange(1, 13000+1, 200)\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(x, loss_list, x , dice_list)\n",
    "plt.ylim(0, 1)\n",
    "plt.yticks(np.arange(1.1, step=0.1))\n",
    "plt.legend((\"loss\", \"dice\"))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
